{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume NER\n",
    "## Extract Information from Resumes using NER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Data Exploration and preprocessing\n",
    "In this first part of the challenge, we will load and examine the dataset we will be working with. We will also prepare the data for training which we will start in the second part of the challenge. You will be required to program some basic python pertaining to file loading, data conversion, and basic dictionaries and array manipulation. If you are experienced with Python, this will be easy. If you are new to python and/or programming, it will be a good opportunity to learn some basic programming you will need for data loading and exploration.\n",
    "\n",
    "* *If you need help setting up python or running this notebook, please get help from the  assistants to the professor*\n",
    "* *It might be helpful to try your code out first in a python ide like pycharm before copying it an running it here in this notebook*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset\n",
    "The dataset we will be using is located in the dataset folder included in the project. Verify the data is available by executing the code cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataset_path = \"../dataset/Entity Recognition in Resumes.json\"\n",
    "print(\"Path exists? {}\".format(os.path.exists(dataset_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good? OK then let's load the dataset. The dataset is structured so that each line of text is a resume. \n",
    "You will do the following:\n",
    "1. using python's built-in \"open\" function, get a filehandle to the dataset (tip don't forget the file is utf8!)\n",
    "2. load the data into an array of resumes (each text line is one resume) \n",
    "3. use the print function to print how many resumes were loaded\n",
    "4. use the print function to output one of the resumes so we can see how the resumes look in raw text form \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701 Lebensläufe geladen..\n",
      "\n",
      "Sample resume:\n"
     ]
    }
   ],
   "source": [
    "## use the \"open\" function to get a filehandle. \n",
    "with open(\"../dataset/Entity Recognition in Resumes.json\",encoding=\"utf8\") as f:\n",
    "    ## use the filehandle to read all lines into an array of text lines. \n",
    "    lines = f.readlines()\n",
    "    ## print how many lines were loaded\n",
    "    print(\"{} Lebensläufe geladen..\".format(len(lines)))  \n",
    "    ## now print one resume/line to see how the resumes look in raw text form\n",
    "    print()\n",
    "    print(\"Sample resume:\")\n",
    "    #print(lines[81])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the dataset to json\n",
    "As we can see, the resumes are not in a convenient human-readable form, but are json dictionaries. We want to work with the resumes as python dictionaries and not as raw text, so we will convert the resumes from text to dictionaries. We will do the following:\n",
    "1. Import the json module\n",
    "2. Loop through all of the text lines and use the json 'loads' function to convert the line to a python dictionary. Tip - you can use a 'for' loop, or if you want to get fancy, a python 'list comprehension' to accomplish this. \n",
    "3. Select one of the converted resumes so that we can examine its structure.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import json module to load json strings\n",
    "import json\n",
    "## using a for loop or a list comprehension, cycle through all lines (loaded above) and convert them to dictionaries \n",
    "## using json loads function. Make sure all converted resumes are stored in the 'all_resumes' array below  \n",
    "all_resumes = [json.loads(s) for s in lines]\n",
    "## select one resume to examine from the all_resumes list\n",
    "resume = all_resumes[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore the resume data structure\n",
    "You should have one sample resume saved in the \"resume\" variable. Now we will examine the resume dictionary. Execute the code cell below to see the keys in the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys in resume:\n",
      "['content', 'annotation', 'extras', 'metadata']\n",
      "\n",
      "Typ of key 'content'' is <class 'str'> \n",
      "Typ of key 'annotation'' is <class 'list'> \n",
      "Typ of key 'extras'' is <class 'NoneType'> \n",
      "Typ of key 'metadata'' is <class 'dict'> \n"
     ]
    }
   ],
   "source": [
    "## explore keys in cv\n",
    "print(\"keys in resume:\")\n",
    "print(list(resume.keys()))\n",
    "print()\n",
    "for key in resume.keys():\n",
    "    print(\"Typ of key '{}'' is {} \".format(key,type(resume[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: which key do you think points to the text content of the resume?\n",
    "*Answer here*\n",
    "##### Question: which key do you think points to the list of entity annotations? \n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your answers above, see if you were right by printing the text content and the entity list by completing and executing the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume content:\n",
      "Navas Koya\n",
      "Test Engineer\n",
      "\n",
      "Mangalore, Karnataka - Email me on Indeed: indeed.com/r/Navas-Koya/23c1e4e94779b465\n",
      "\n",
      "Willing to relocate to: Mangalore, Karnataka - Bangalore, Karnataka - Chennai, Tamil Nadu\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "System Engineer\n",
      "\n",
      "Infosys -\n",
      "\n",
      "August 2014 to Present\n",
      "\n",
      ".NET application Maintenance and do the code changes if required\n",
      "\n",
      "Test Engineer\n",
      "\n",
      "Infosys -\n",
      "\n",
      "June 2015 to February 2016\n",
      "\n",
      "PrProject 2:\n",
      "\n",
      "Title: RBS W&G Proving testing.\n",
      "Technology: Manual testing\n",
      "Role: Software Test Engineer\n",
      "\n",
      "Domain: Banking\n",
      "Description:\n",
      "\n",
      "Write test cases & descriptions. Review the entries. Upload and map the documents into\n",
      "HP QC. Execute the testing operations in TPROD mainframe. Upload the result in QC along with\n",
      "the proof.\n",
      "Roles and Responsibilities:\n",
      "•Prepared the Test Scenarios\n",
      "\n",
      "•Prepared and Executed Test Cases\n",
      "•Performed functional, Regression testing, Sanity testing.\n",
      "\n",
      "•Reviewed the Test Reports and Preparing Test Summary Report.\n",
      "•Upload Test cases to the QC.\n",
      "•Execute in TPROD Mainframe.\n",
      "•Defect Track and Report.\n",
      "\n",
      "Test Executive\n",
      "\n",
      "Infosys Limited -\n",
      "\n",
      "August 2014 to May 2015\n",
      "\n",
      "https://www.indeed.com/r/Navas-Koya/23c1e4e94779b465?isid=rex-download&ikw=download-top&co=IN\n",
      "\n",
      "\n",
      "Project 1:\n",
      "Title: CAWP (Compliance Automated Work Paper)\n",
      "\n",
      "Technology: Manual testing\n",
      "Role: Software Test Executive\n",
      "Domain: Banking\n",
      "Description:\n",
      "The Admin can create and maintain annual test plan, and users can only view and add\n",
      "details. Testers will get Business Requirement which explains the flows and Functional\n",
      "requirements which gives the full detail of the project.\n",
      "Roles and Responsibilities:\n",
      "\n",
      "•Prepared the Test Scenarios\n",
      "•Prepared and Executed Test Cases\n",
      "•Performed functional, Regression testing, Sanity testing.\n",
      "•Reviewed the Test Reports and Preparing Test Summary Report.\n",
      "•Defect Track and Report.\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "Bachelor of Computer Applications\n",
      "\n",
      "Mangalore University, Mangalore\n",
      "\n",
      "June 2011 to April 2014\n",
      "\n",
      "SKILLS\n",
      "\n",
      "C# (Less than 1 year), .NET, SQL Server, Css, Html5\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "Bachelor of computer application: with 74% from Milagres College, Kallianpur under\n",
      "Mangalore University, Karnataka.\n",
      "\n",
      "Navas Najeer Koya 2\n",
      "\n",
      "SKILL SET • ASP.NET, C# • QA tools\n",
      "\n",
      "• Coding and modularization • Excellent communication skills\n",
      "\n",
      "• VB, VB.net, ASP • Technical specifications creation\n",
      "\n",
      "• HTML • System backups\n",
      "\n",
      "• Sql server 2005, Oracle • System upgrades\n",
      "\n",
      "• Java/C/C++ • Excellent problem-solving abilities\n",
      "\n",
      "Navas Najeer Koya 3\n",
      "resume entity list:\n",
      "[{'label': ['Skills'], 'points': [{'start': 2110, 'end': 2403, 'text': 'SKILL SET • ASP.NET, C# • QA tools\\n\\n• Coding and modularization • Excellent communication skills\\n\\n• VB, VB.net, ASP • Technical specifications creation\\n\\n• HTML • System backups\\n\\n• Sql server 2005, Oracle • System upgrades\\n\\n• Java/C/C++ • Excellent problem-solving abilities\\n\\nNavas Najeer Koya 3'}]}, {'label': ['Location'], 'points': [{'start': 2055, 'end': 2063, 'text': 'Mangalore'}]}, {'label': ['Skills'], 'points': [{'start': 1895, 'end': 1946, 'text': 'C# (Less than 1 year), .NET, SQL Server, Css, Html5\\n'}]}, {'label': ['Graduation Year'], 'points': [{'start': 1880, 'end': 1884, 'text': ' 2014'}]}, {'label': ['Location'], 'points': [{'start': 1851, 'end': 1859, 'text': 'Mangalore'}]}, {'label': ['Location'], 'points': [{'start': 1829, 'end': 1837, 'text': 'Mangalore'}]}, {'label': ['Degree'], 'points': [{'start': 1794, 'end': 1825, 'text': 'Bachelor of Computer Application'}]}, {'label': ['Graduation Year'], 'points': [{'start': 1056, 'end': 1060, 'text': ' 2014'}]}, {'label': ['Companies worked at'], 'points': [{'start': 1031, 'end': 1037, 'text': 'Infosys'}]}, {'label': ['Designation'], 'points': [{'start': 479, 'end': 492, 'text': 'Test Engineer\\n'}]}, {'label': ['Companies worked at'], 'points': [{'start': 352, 'end': 358, 'text': 'Infosys'}]}, {'label': ['Designation'], 'points': [{'start': 337, 'end': 350, 'text': 'Test Engineer\\n'}]}, {'label': ['Graduation Year'], 'points': [{'start': 253, 'end': 257, 'text': ' 2014'}]}, {'label': ['Companies worked at'], 'points': [{'start': 236, 'end': 242, 'text': 'Infosys'}]}, {'label': ['Designation'], 'points': [{'start': 219, 'end': 233, 'text': 'System Engineer'}]}, {'label': ['Location'], 'points': [{'start': 135, 'end': 143, 'text': 'Mangalore'}]}, {'label': ['Location'], 'points': [{'start': 26, 'end': 34, 'text': 'Mangalore'}]}, {'label': ['Designation'], 'points': [{'start': 11, 'end': 24, 'text': 'Test Engineer\\n'}]}, {'label': ['Name'], 'points': [{'start': 0, 'end': 9, 'text': 'Navas Koya'}]}]\n"
     ]
    }
   ],
   "source": [
    "## print the resume text\n",
    "print(\"resume content:\")\n",
    "print(resume[\"content\"])\n",
    "## print the resume's list of entity annotations\n",
    "print(\"resume entity list:\")\n",
    "print(resume[\"annotation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore the list of entity labels\n",
    "The entity list is a list of dictionaries, we want to explore this list\n",
    "1. Cycle through the entities in the list. You can use a 'for' loop for this\n",
    "2. For each entity - which will be a dictionary - print out each key and each value for the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 'label' Value: ['Skills']\n",
      "Key: 'points' Value: [{'start': 2110, 'end': 2403, 'text': 'SKILL SET • ASP.NET, C# • QA tools\\n\\n• Coding and modularization • Excellent communication skills\\n\\n• VB, VB.net, ASP • Technical specifications creation\\n\\n• HTML • System backups\\n\\n• Sql server 2005, Oracle • System upgrades\\n\\n• Java/C/C++ • Excellent problem-solving abilities\\n\\nNavas Najeer Koya 3'}]\n",
      "\n",
      "Key: 'label' Value: ['Location']\n",
      "Key: 'points' Value: [{'start': 2055, 'end': 2063, 'text': 'Mangalore'}]\n",
      "\n",
      "Key: 'label' Value: ['Skills']\n",
      "Key: 'points' Value: [{'start': 1895, 'end': 1946, 'text': 'C# (Less than 1 year), .NET, SQL Server, Css, Html5\\n'}]\n",
      "\n",
      "Key: 'label' Value: ['Graduation Year']\n",
      "Key: 'points' Value: [{'start': 1880, 'end': 1884, 'text': ' 2014'}]\n",
      "\n",
      "Key: 'label' Value: ['Location']\n",
      "Key: 'points' Value: [{'start': 1851, 'end': 1859, 'text': 'Mangalore'}]\n",
      "\n",
      "Key: 'label' Value: ['Location']\n",
      "Key: 'points' Value: [{'start': 1829, 'end': 1837, 'text': 'Mangalore'}]\n",
      "\n",
      "Key: 'label' Value: ['Degree']\n",
      "Key: 'points' Value: [{'start': 1794, 'end': 1825, 'text': 'Bachelor of Computer Application'}]\n",
      "\n",
      "Key: 'label' Value: ['Graduation Year']\n",
      "Key: 'points' Value: [{'start': 1056, 'end': 1060, 'text': ' 2014'}]\n",
      "\n",
      "Key: 'label' Value: ['Companies worked at']\n",
      "Key: 'points' Value: [{'start': 1031, 'end': 1037, 'text': 'Infosys'}]\n",
      "\n",
      "Key: 'label' Value: ['Designation']\n",
      "Key: 'points' Value: [{'start': 479, 'end': 492, 'text': 'Test Engineer\\n'}]\n",
      "\n",
      "Key: 'label' Value: ['Companies worked at']\n",
      "Key: 'points' Value: [{'start': 352, 'end': 358, 'text': 'Infosys'}]\n",
      "\n",
      "Key: 'label' Value: ['Designation']\n",
      "Key: 'points' Value: [{'start': 337, 'end': 350, 'text': 'Test Engineer\\n'}]\n",
      "\n",
      "Key: 'label' Value: ['Graduation Year']\n",
      "Key: 'points' Value: [{'start': 253, 'end': 257, 'text': ' 2014'}]\n",
      "\n",
      "Key: 'label' Value: ['Companies worked at']\n",
      "Key: 'points' Value: [{'start': 236, 'end': 242, 'text': 'Infosys'}]\n",
      "\n",
      "Key: 'label' Value: ['Designation']\n",
      "Key: 'points' Value: [{'start': 219, 'end': 233, 'text': 'System Engineer'}]\n",
      "\n",
      "Key: 'label' Value: ['Location']\n",
      "Key: 'points' Value: [{'start': 135, 'end': 143, 'text': 'Mangalore'}]\n",
      "\n",
      "Key: 'label' Value: ['Location']\n",
      "Key: 'points' Value: [{'start': 26, 'end': 34, 'text': 'Mangalore'}]\n",
      "\n",
      "Key: 'label' Value: ['Designation']\n",
      "Key: 'points' Value: [{'start': 11, 'end': 24, 'text': 'Test Engineer\\n'}]\n",
      "\n",
      "Key: 'label' Value: ['Name']\n",
      "Key: 'points' Value: [{'start': 0, 'end': 9, 'text': 'Navas Koya'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## explore entity list\n",
    "for entity in resume[\"annotation\"]:\n",
    "    for key in entity:\n",
    "        print(\"Key: '{}' Value: {}\".format(key, entity[key]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: What keys do the entity entries have? What is the datatype of the values of these keys?\n",
    "*Answer here*\n",
    "##### Question: What do these keys and values mean? (think of their significance as entity labels)\n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert  data to \"spacy\" offset format\n",
    "Before we go any further, we need to convert the data into a slightly more compact format. This format is the format we will be using to train our first models in the next part of the challenge. Here we will do the following:\n",
    "1. Import the data conversion function\n",
    "2. Convert the data with that function, storing the results in a variable\n",
    "3. Inspect the converted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted resumes has 701 resumes.\n"
     ]
    }
   ],
   "source": [
    "## define data conversion method\n",
    "def convert_data(data):\n",
    "    \"\"\"\n",
    "    Creates NER training data in Spacy format from JSON dataset\n",
    "    Outputs the Spacy training data which can be used for Spacy training.\n",
    "    \"\"\"\n",
    "    text = data['content']\n",
    "    entities = []\n",
    "    if data['annotation'] is not None:\n",
    "        for annotation in data['annotation']:\n",
    "            # only a single point in text annotation.\n",
    "            point = annotation['points'][0]\n",
    "            labels = annotation['label']\n",
    "            # handle both list of labels or a single label.\n",
    "            if not isinstance(labels, list):\n",
    "                labels = [labels]\n",
    "            for label in labels:\n",
    "                # dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
    "                entities.append((point['start'], point['end'] + 1, label))\n",
    "    return (text, {\"entities\": entities})\n",
    "   \n",
    "## using a loop or list comprehension, convert each resume in all_resumes using the convert function above, storing the result\n",
    "converted_resumes = [convert_data(resume) for resume in all_resumes]\n",
    "## print the number of resumes in converted resumes \n",
    "print(\"Converted resumes has {} resumes.\".format(len(converted_resumes)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: how is the converted data different than the original data? How is it the same? \n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filter out resumes without annotations\n",
    "A few of the resumes have an empty entity list. We want to filter these resumes out of our data before continuing. We will do the following:\n",
    "1. cycle through all resumes using for loop or list comprehension\n",
    "2. for each resume, if the resume has no labled entities, ignore it. Otherwise save it to new resume list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690\n"
     ]
    }
   ],
   "source": [
    "## filter out resumes where resume entities list is None (you can do this in a one-line list comprehension)\n",
    "## sove to converted_resumes variable\n",
    "converted_resumes = [res for res in converted_resumes if res[1]['entities']]\n",
    "## print length of new filtered converted_resumes.  \n",
    "print(len(converted_resumes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print all entities for one converted resume\n",
    "The converted data also has an entity list. You should be able to examine this using similar techniques we have used above on the converted data. In the next code block you will write code that will print all of the entities for one resume. TIP each entity entry in the 'entities' list consists of a start index of the entity in the resume text, an end index, and the entity label. We will do the following:\n",
    "1. Store one converted resume in the 'converted_resume' variable\n",
    "2. Find the entity list in the converted_resume\n",
    "3. Cycle through the entities, and - using the start and end index - print the label of the entity and the value of the entity. This will be the text substring pointed to by the start and end index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Skills': 'CRM (10+ years), CUSTOMER RELATIONSHIP MANAGEMENT (10+ years), TESTING (10+ years),\n",
      "UI (10+ years), USER INTERFACE (10+ years)\n",
      "'\n",
      "\n",
      "'Graduation Year': '2003'\n",
      "\n",
      "'College Name': 'St. Joseph's convent school\n",
      "'\n",
      "\n",
      "'Graduation Year': '2009'\n",
      "\n",
      "'College Name': 'Padmanava College of Engineering\n",
      "'\n",
      "\n",
      "'Degree': 'B.E in CSE\n",
      "'\n",
      "\n",
      "'Companies worked at': 'SAP Labs '\n",
      "\n",
      "'Companies worked at': 'SAP Labs '\n",
      "\n",
      "'Companies worked at': 'SAP Labs '\n",
      "\n",
      "'Designation': 'Quality Engineer'\n",
      "\n",
      "'Companies worked at': 'SAP AG'\n",
      "\n",
      "'Designation': 'Offshore SAP CRM Functional Consultant\n",
      "'\n",
      "\n",
      "'Email Address': 'Indeed: indeed.com/r/Shabnam-Saba/dc70fc366accb67f'\n",
      "\n",
      "'Location': 'Bengaluru'\n",
      "\n",
      "'Designation': 'Offshore SAP CRM Functional Consultant\n",
      "'\n",
      "\n",
      "'Name': 'Shabnam Saba'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## store a resume in the variable\n",
    "converted_resume = converted_resumes[73]\n",
    "## find text content and store in variable\n",
    "text = converted_resume[0]\n",
    "## find the entities list and store in variable\n",
    "entities_list = converted_resume[1]['entities']\n",
    "## for each entity, print the label, and the text (text content substring pointed to by start and end index)\n",
    "for entity in entities_list:\n",
    "    print(\"'{}': '{}'\".format(entity[2],text[entity[0]:entity[1]]))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: What are some of the entity labels you see? Are there any entity values that seem surprising or particularly interesting? \n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collect unique labels of all entities in dataset\n",
    "Now we are interested in finding out all of the (unique) entity labels which exist in our dataset. Complete and execute the code below to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity labels:  {'Skills', 'Graduation Year', 'Email Address', 'College Name', 'Can Relocate to', 'links', 'Certifications', 'Links', 'Rewards and Achievements', 'UNKNOWN', 'College', 'Location', 'Companies worked at', 'Relocate to', 'Name', 'Years of Experience', 'Degree', 'Designation', 'state', 'des', 'University', 'abc', 'Address', 'projects', 'training'}\n"
     ]
    }
   ],
   "source": [
    "## collect names of all entities in complete resume dataset\n",
    "## create empty list where we will store all entity labels\n",
    "all_labels = list()\n",
    "for res in converted_resumes:\n",
    "    entity_list = res[1][\"entities\"]\n",
    "    all_labels.extend([ent[2] for ent in entity_list])           \n",
    "## all_labels is not yet unique. Make a set to contain only unique values\n",
    "unique_labels = set(all_labels)\n",
    "print(\"Entity labels: \",unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see all entity labels in our dataset. Do some of them seem particularly interesting to you? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose up to 3 Entities from the list that you would like to use for training a named entity recognition model. \n",
    "##### Question: which entities did you choose? \n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate entities\n",
    "Now we need to check that there is adequate training data for the entities you have chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs with Companies worked at: 627\n",
      "Total count of Companies worked at: 2830\n",
      "Docs with Degree: 606\n",
      "Total count of Degree: 1012\n"
     ]
    }
   ],
   "source": [
    "## store entity label names in an array \n",
    "chosen_entity_label = [\"Companies worked at\",\"Degree\"]\n",
    "## for each chosen entity label, count how many documents have a labeled entity for that label, and how many labeled entities total there are \n",
    "## for that entity\n",
    "for chosen in chosen_entity_label:\n",
    "    found_docs_with_entity = 0\n",
    "    entity_count = 0\n",
    "    for resume in converted_resumes:\n",
    "        entity_list = resume[1][\"entities\"]\n",
    "        _,_,labels = zip(*entity_list)\n",
    "        if chosen in labels:\n",
    "            found_docs_with_entity+=1\n",
    "            entity_count+=len([l for l in labels if l == chosen])\n",
    "    print(\"Docs with {}: {}\".format(chosen,found_docs_with_entity))\n",
    "    print(\"Total count of {}: {}\".format(chosen,entity_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Question: Is adequate training data available for the entities you have chosen? (there should be at least several hundered examples total of each entity)\n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save converted data for later use\n",
    "We are almost done with the first part of the challenge! One more detail. We need to save the \"converted_resumes\" list so we can load it in the next notebook. We will do the following:\n",
    "1. Store the location we want to save the data to in the 'converted_resumes_path' variable\n",
    "2. Using python's 'open' function and the 'json' module's 'dump' function, save the data to disk. Make sure to create missing directories (if applicable) using python's \"os.makedirs\" function. Save the file with a \".json\" file extension\n",
    "3. Check the filesystem if the file exists and is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "converted_resumes_path = \"../dataset/converted_resumes.json\"\n",
    "with open(converted_resumes_path,'w+',encoding='utf8') as f:\n",
    "    json.dump(converted_resumes,f)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "We are done with part one. Now we will go on to train our own NER Models with the dataset and the entities we have chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
