{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Resume NER\n",
    "## Extract Information from Resumes using NER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - NER with Spacy\n",
    "In this second part of the challenge, we will be using the preprocessed data from part one to start training NER models. We will be using spacy (https://spacy.io/) here to \"get our feet wet\" with NER, as training spacy can be reasonably done on our laptops and does not yet necessarily require a GPU. Spacy is a powerful, effective, and resource-efficient NLP library - It might surprise us with its performance on the challenge!\n",
    "\n",
    "We will run spacy's pretrained models on our data to get a feel for NER, and then we will perform some additional preprocessing on our data before we start training our own NER model using the labelled entities we have identified in part one. \n",
    "We will also explore evaluation metrics for NER, and decide how we want to quantify the performance of our trained models. \n",
    "\n",
    "* *If you need help setting up python or running this notebook, please get help from the  assistants to the professor*\n",
    "* *It might be helpful to try your code out first in a python ide like pycharm before copying it an running it here in this notebook*\n",
    "* *For solving the programming tasks, use the python reference linked here (Help->Python Reference) as well as Web-searches.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reload preprocessed data\n",
    "Here, we will load the data we saved in part one and save it to a variable. Provide code below to load the data and store it as a list in a variable. (Hint - use 'open' and the json module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 690 resumes\n"
     ]
    }
   ],
   "source": [
    "## import json module\n",
    "import json\n",
    "path = \"../dataset/converted_resumes.json\"\n",
    "## TODO open file and load as json\n",
    "with open(path,encoding=\"utf8\") as f:\n",
    "    resumes = json.load(f)\n",
    "## TODO print length of loaded resumes list to be sure everything ok\n",
    "print(\"Loaded: {} resumes\".format(len(resumes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Take Spacy for a spin\n",
    "Before we train our own NER model to recognize the resume-specific entities we want to capture, let's see how spacy's pretrained NER models do on our data. These pretrained models can't recognize our entities yet, but let's see how they do. Run the next code block to load spacy's English language model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x00000279710F6E10>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "print(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the EntityRecognizer in the loaded nlp pipeline and display the labels it supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LANGUAGE', 'WORK_OF_ART', 'MONEY', 'PERSON', 'EVENT', 'CARDINAL', 'ORDINAL', 'QUANTITY', 'DATE', 'ORG', 'NORP', 'FAC', 'LAW', 'TIME', 'PERCENT', 'PRODUCT', 'GPE', 'LOC')\n"
     ]
    }
   ],
   "source": [
    "ner = nlp.get_pipe('ner')\n",
    "labels = ner.labels\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: What do the 'GPE', 'FAC' and 'NORP' labels stand for? (Tipp: use either the spacy.explain method, or google the spacy.io api docs) \n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGUAGE:  Any named language\n",
      "WORK_OF_ART:  Titles of books, songs, etc.\n",
      "MONEY:  Monetary values, including unit\n",
      "PERSON:  People, including fictional\n",
      "EVENT:  Named hurricanes, battles, wars, sports events, etc.\n",
      "CARDINAL:  Numerals that do not fall under another type\n",
      "ORDINAL:  \"first\", \"second\", etc.\n",
      "QUANTITY:  Measurements, as of weight or distance\n",
      "DATE:  Absolute or relative dates or periods\n",
      "ORG:  Companies, agencies, institutions, etc.\n",
      "NORP:  Nationalities or religious or political groups\n",
      "FAC:  Buildings, airports, highways, bridges, etc.\n",
      "LAW:  Named documents made into laws.\n",
      "TIME:  Times smaller than a day\n",
      "PERCENT:  Percentage, including \"%\"\n",
      "PRODUCT:  Objects, vehicles, foods, etc. (not services)\n",
      "GPE:  Countries, cities, states\n",
      "LOC:  Non-GPE locations, mountain ranges, bodies of water\n"
     ]
    }
   ],
   "source": [
    "### if you choose to use spacy's 'explain' method to get the answer to the question above, provide your code here\n",
    "for label in labels:\n",
    "    print(\"{}:  {}\".format(label,spacy.explain(label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the entities are different than the entities we will train our custom model on. \n",
    "##### Question: what entities do you think this model will find in an example resume?\n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will work with one of our resumes, and get spacy to tell us what entities it recognizes. Complete the code block below to get a single resume text out of our resume list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nitin Verma\n",
      "Assisting Microsoft Partners - Exchange Online and Office\n",
      "Pune, Maharashtra - Email me on Indeed: indeed.com/r/Nitin-Verma/b9e8520147f728d2\n",
      "WORK EXPERIENCE\n",
      "Assisting Microsoft Partners\n",
      "Exchange Online and Office -\n",
      "September 2017 to Present\n",
      "from around the world with AD, Exchange Online and Office 365\n",
      "related issues.\n",
      "➢ Prompt assistance within the required SLA as per the case creation by users and follow up\n",
      "timely till the issue has been resolved up to the user's satisfaction.\n",
      "➢ Troubleshooting and resolving various production impacting critical issues.\n",
      "➢ Educating Admins about various Microsoft Office 365 features and assisting them with\n",
      "implementing the same using GUI as well as PowerShell.\n",
      "➢ Reproducing the users' issues on test environment and researching to find a resolution.\n",
      "➢ Interact with other 2nd level support team for joined troubleshooting sessions where root cause\n",
      "is\n",
      "not well defined.\n",
      "➢ Assisting users/admins with various Office 365 applications, like Outlook, Word, SharePoint,\n",
      "OneDrive, Skype For Business, etc.\n",
      "➢ Brainstorming with the Technical Advisors from Microsoft regarding Service Incidents impacting\n",
      "multiple tenants.\n",
      "Tools Used: RAVE, CAP and AVAYA.\n",
      "EDUCATION\n",
      "Bachelor of Engineering in EnTC\n",
      "Pune University -  Pune, Maharashtra\n",
      "https://www.indeed.com/r/Nitin-Verma/b9e8520147f728d2?isid=rex-download&ikw=download-top&co=IN\n"
     ]
    }
   ],
   "source": [
    "### TODO get a single resume text and print it out\n",
    "restxt = resumes[200][0]\n",
    "print(\"\\n\".join(restxt.split('\\n\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting entities with spacy is easy with a pretrained model. We simply call the model (here 'nlp') with our text to get a spacy Document. See https://spacy.io/api/doc for more detail. \n",
    "\n",
    "Execute the code below to process the resume txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(restxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The doc object has a list of entities predicted by spacy 'ents'. We would like to loop through all of these entities and print their label and associated text to see what spacy predicted for this resume.\n",
    "\n",
    "Complete the code below to do this. You will probably need to google the spacy api docs to find the solution (Tipp: look for 'Doc.ents'). Also, trying code in your ide (for example pycharm) before copying it here might help with exploring and debugging to find the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.span.Span'>\n",
      "PERSON     Nitin Verma\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        Microsoft Partners - Exchange Online\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        Maharashtra - Email\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        Exchange Online\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        Office 365\n",
      "\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "CARDINAL   ➢\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        SLA\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        ➢\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        Microsoft Office 365\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        GUI\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        PowerShell\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORDINAL    2nd\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "CARDINAL   ➢\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "LAW        Office 365\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "GPE        Outlook\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        SharePoint\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        Skype For Business\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        the Technical Advisors\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        Microsoft\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        Service Incidents\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        CAP\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        AVAYA\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        EnTC\n",
      "\n",
      "Pune University -  Pune\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "ORG        Maharashtra\n"
     ]
    }
   ],
   "source": [
    "##TODO loop through the doc's entities, and print the label and text for each entity found. \n",
    "#print(doc.ents)\n",
    "for ent in doc.ents:\n",
    "    print(type(ent))    \n",
    "    print(\"{:10} {}\".format(ent.label_,ent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions: What is your first impression of spacy's NER based on the results above? Does it seem accurate/powerfull?\n",
    "##### Does it make many mistakes? Do some entity types seem more accurate than others? \n",
    "*Answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as a comparison, we will list the entities contained in the resume's original annotated training data (remember, the existing annotations were created by a human-annotator, and not predicted by a machine like the entities predicted above) \n",
    "\n",
    "Complete the code below to do the following: \n",
    "* Access the 'entities' list of the example resume you chose, loop through the entities and print them out. \n",
    "* *Tip: one entity in the list is a tuple with the following structure: (12,1222,\"label\") where the first element is the start index of the entity in the resume text, the second element is the end index, and the third element is the label.\n",
    "* Use this Tip to print out a formatted list of entities \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Address        indeed.com/r/Nitin-Verma/b9e8520147f728d2\n",
      "Location             Pune\n",
      "Companies worked at  Pune University\n",
      "Location             Pune\n",
      "Degree               Bachelor of Engineering in EnTC\n",
      "Email Address        indeed.com/r/Nitin-Verma/b9e8520147f728d2\n",
      "Location             Pune\n",
      "Designation          Assisting Microsoft Partners - Exchange Online and Office\n",
      "Name                 Nitin Verma\n"
     ]
    }
   ],
   "source": [
    "##TODO access entities\n",
    "res = resumes[200]\n",
    "restext = res[0] \n",
    "labeled_ents = res[1]['entities']\n",
    "## TDOD print out formatted list of entities\n",
    "for ent in labeled_ents:\n",
    "    enttext = restext[ent[0]:ent[1]]\n",
    "    enttext = \" \".join(enttext.split())\n",
    "    print(\"{:20} {}\".format(ent[2],enttext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already know, the annotated entities in the training data are different than the entities spacy can recognize with it's pretrainied models, so we need to train a custom NER model. We will get started with that now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Training Data for NER model training\n",
    "We need to do some more preprocessing of our training data before we can train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the entity labels you chose in part 1 of the challenge? We will be training a model to predict those entities.\n",
    "As a first step, we will gather all resumes that contain at least one training annotation for those entities.\n",
    "\n",
    "Complete and execute the code below to gather your training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered 547 training examples\n"
     ]
    }
   ],
   "source": [
    "##TODO Store the entity labels you want to train for as array in chosen_entity_labels\n",
    "#chosen_entity_labels = [\"Companies worked at\",\"Degree\",\"Skills\",\"Designation\"]\n",
    "chosen_entity_labels = [\"Companies worked at\",\"Degree\",\"Designation\"]\n",
    "\n",
    "## this method gathers all resumes which have all of the chosen entites above.\n",
    "def gather_candidates(dataset,entity_labels):\n",
    "    candidates = list()\n",
    "    for resume in dataset:\n",
    "        res_ent_labels = list(zip(*resume[1][\"entities\"]))[2]\n",
    "        if set(entity_labels).issubset(res_ent_labels):\n",
    "            candidates.append(resume)\n",
    "    return candidates\n",
    "## TODO use the gather candidates methods and store result in training_data variable\n",
    "training_data = gather_candidates(resumes,chosen_entity_labels)\n",
    "print(\"Gathered {} training examples\".format(len(training_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have those training examples which contain the entities we are interested in. Do you have at least a few hundred examples? If not, you might need to re-think the entities you chose or try just one or two of them and re-run the notebooks. It is important that we have several hundred examples for training (e.g. more than 200. 3-500 is better). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove other entity annotations from training data\n",
    "Now that we have our training data, we want to remove all but relevant (chosen) entity annotations from this data, so that the model we train will only train for our entities. Complete and execute the code below to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter all annotation based on filter list\n",
    "def filter_ents(ents, filter):\n",
    "    filtered = [ent for ent in ents if ent[2] in filter]\n",
    "    return filtered\n",
    "\n",
    "## now remove all but relevant (chosen) entity annotations and store in X variable \n",
    "X = [[dat[0], dict(entities=filter_ents(dat[1]['entities'], chosen_entity_labels))] for dat in training_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove resumes that cause errors in spacy\n",
    "Depending on what entities you chose, some of the resumes might cause errors in spacy. We don't need to get into details as to why, suffice to say it has to do with whitespace and syntax in the entity annotations. If these resumes are not removed from our training data, spacy will throw an exception during training, so we need to remove them first. \n",
    "\n",
    "We will use the remove_bad_data function below to do this. This function does the following:\n",
    "* calls train_spacy_ner with debug=True and n_iter=1. This causes spacy to process the documents one-by-one, and gather the documents that throw an exception in a list of \"bad docs\" which it returns. \n",
    "* You will complete the function to remove any baddocs (returned by remove_bad_data) from your training data list. \n",
    "\n",
    "You may or may not have any bad documents depending on the entities you chose. In any case, there should not be more than a dozen or so bad docs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Exception thrown when processing doc:\n",
      "('Nida Khan\\nTech Support Executive - Teleperformance for Microsoft\\n\\nJaipur, Rajasthan - Email me on Indeed: indeed.com/r/Nida-Khan/6c9160696f57efd8\\n\\n• To be an integral part of the organization and enhance my knowledge to utilize it in a productive\\nmanner for the growth of the company and the global.\\n\\nINDUSTRIAL TRAINING\\n\\n• BHEL, (HEEP) HARIDWAR\\nOn CNC System&amp; PLC Programming.\\n\\nWORK EXPERIENCE\\n\\nTech Support Executive\\n\\nTeleperformance for Microsoft -\\n\\nSeptember 2017 to Present\\n\\nprocess.\\n• 21 months of experience in ADFC as Phone Banker.\\n\\nEDUCATION\\n\\nBachelor of Technology in Electronics & communication Engg\\n\\nGNIT institute of Technology -  Lucknow, Uttar Pradesh\\n\\n2008 to 2012\\n\\nClass XII\\n\\nU.P. Board -  Bareilly, Uttar Pradesh\\n\\n2007\\n\\nClass X\\n\\nU.P. Board -  Bareilly, Uttar Pradesh\\n\\n2005\\n\\nSKILLS\\n\\nMicrosoft office, excel, cisco, c language, cbs. (4 years)\\n\\nhttps://www.indeed.com/r/Nida-Khan/6c9160696f57efd8?isid=rex-download&ikw=download-top&co=IN',) ({'entities': [[552, 610, 'Degree'], [420, 449, 'Companies worked at'], [395, 418, 'Designation'], [35, 64, 'Companies worked at'], [10, 33, 'Designation'], [9, 32, 'Designation']]},)\n",
      "Losses {'ner': 47445.27099112627}\n",
      "Unfiltered training data size:  547\n",
      "Filtered training data size:  546\n",
      "Bad data size:  1\n"
     ]
    }
   ],
   "source": [
    "from spacy_train_resume_ner import train_spacy_ner\n",
    "\n",
    "def remove_bad_data(training_data):\n",
    "    model, baddocs = train_spacy_ner(training_data, debug=True, n_iter=1)\n",
    "    ## training data is list of lists with each list containing a text and annotations\n",
    "    ## baddocs is a set of strings/resume texts.\n",
    "    ## TODO complete implementation to filter bad docs and store filter result (good docs) in filtered variable\n",
    "    filtered = [data for data in training_data if data[0] not in baddocs]\n",
    "    print(\"Unfiltered training data size: \",len(training_data))\n",
    "    print(\"Filtered training data size: \", len(filtered))\n",
    "    print(\"Bad data size: \", len(baddocs))\n",
    "    return filtered\n",
    "\n",
    "## call remove method. It may take a few minutes for the method to complete.\n",
    "## you will know it is complete when the print output above. \n",
    "X = remove_bad_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: How many bad docs did you have? What is the size of your new (filtered) training data? \n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Test Split\n",
    "Now before we train our model, we have to split our available training data into training and test sets. Splitting our data into train and test (or holdout) datasets is a fundamental technique in machine learning, and essential to avoid the problem of overfitting.\n",
    "Before we go on, you should get a grasp of how train/test split helps us avoid overfitting. Please take the time now to do a quick web search on the topic. There are many resources available. You should search for \"train test validation overfitting\" or some subset of those terms.\n",
    "\n",
    "Here are a few articles to start with:\n",
    "* https://machinelearningmastery.com/a-simple-intuition-for-overfitting/\n",
    "* https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation (Note: you are free to install scikit learn and use the train_test_split method documented here, but it is not necessary. It is the concept that is important)\n",
    "\n",
    "##### Question: What is overfitting and how does doing a train/test split help us avoid overfitting when training our models? Please answer in your own words. \n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand why we do a train/test split, we will write some code that splits our data into train and test sets. Usually we want around 70-80% of the data for train, and the rest for test. \n",
    "##### TODO: Complete the code below to create a train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  382\n",
      "Test size:  164\n",
      "X size:  546\n"
     ]
    }
   ],
   "source": [
    "##TODO complete the implementation  of the train test split function below\n",
    "def train_test_split(X,train_percent):\n",
    "    train_size = int(len(X)*train_percent)\n",
    "    train = X[:train_size]\n",
    "    test = X[train_size:]\n",
    "    assert len(train)+len(test)==len(X)\n",
    "    return train,test\n",
    "## do train test split\n",
    "train,test = train_test_split(X,0.7)\n",
    "## TODO print the size of train and test sets. Do they add up to length of X? \n",
    "print(\"Train size: \",len(train))\n",
    "print(\"Test size: \",len(test))\n",
    "print(\"X size: \",len(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train a spacy ner model with our training data\n",
    "OK, now it is (finally) time to train our own custom NER model using spacy. Because our training data has been preprocessed to only include annotations for the entities we are interested in, the model will only be able to predict/extract those entities. \n",
    "*Depending on your computer, this step may take a while.* Training 20 epochs (iterations) using 480 training examples takes around 10 minutes on my machine (core i7 CPU). You will see output like *Losses {'ner':2342.23342342}* after each epoch/iteration. The default number of iterations is 20, so you will see this output 20 times. When this step is done, we will use the trained ner model to perform predictions on our test data in our test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 32713.253494399753}\n",
      "Losses {'ner': 23305.12905145758}\n",
      "Losses {'ner': 26546.211569716674}\n",
      "Losses {'ner': 27359.807791995176}\n",
      "Losses {'ner': 20706.761624631232}\n",
      "Losses {'ner': 10920.030289953425}\n",
      "Losses {'ner': 8531.85935118602}\n",
      "Losses {'ner': 7412.6600355601}\n",
      "Losses {'ner': 6197.301274820742}\n",
      "Losses {'ner': 5917.083655205084}\n",
      "Losses {'ner': 6195.5344198598395}\n",
      "Losses {'ner': 6062.697890295716}\n",
      "Losses {'ner': 5489.411260393779}\n",
      "Losses {'ner': 5565.391565784825}\n",
      "Losses {'ner': 5471.040000282198}\n",
      "Losses {'ner': 4583.90002346843}\n",
      "Losses {'ner': 4459.246855524312}\n",
      "Losses {'ner': 4549.130778449219}\n",
      "Losses {'ner': 4473.899327961381}\n",
      "Losses {'ner': 4306.245351918046}\n"
     ]
    }
   ],
   "source": [
    "## run this code to train a ner model using spacy\n",
    "custom_nlp,_= train_spacy_ner(train,n_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect NER predictions on one sample resume\n",
    "Now that we have a trained model, let's see how it works on one of our resumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO fetch one resume out of our test dataset and store to the \"resume\" variable\n",
    "resume = test[73]\n",
    "## TODO create a spacy doc out of the resume using our trained model and save to the \"doc\" variable \n",
    "doc = custom_nlp(resume[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will output the predicted entities and the existing annotated entities in that doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED:\n",
      "Designation          PeopleSoft Consultant\n",
      "Degree               e-commerce\n",
      "Degree               e-commerce\n",
      "Designation          PeopleSoft consultant\n",
      "Companies worked at  Oracle\n",
      "Companies worked at  Oracle\n",
      "Degree               e-commerce portal.\n",
      "Degree               Btech information science in BCET\n",
      "Degree               e-commerce\n",
      "\n",
      "LABELED:\n",
      "Degree               e-commerce\n",
      "Degree               Btech information science in BCET\n",
      "\n",
      "Degree               e-commerce\n",
      "Companies worked at  Oracle\n",
      "Companies worked at  Oracle\n",
      "Designation          \n",
      "PeopleSoft consultant\n",
      "\n",
      "Degree               e-commerce\n",
      "Degree               e-commerce\n"
     ]
    }
   ],
   "source": [
    "## TODO output predicted entities (in \"ents\" variable of the spacy doc created above)\n",
    "print(\"PREDICTED:\")\n",
    "for ent in doc.ents:\n",
    "    print(\"{:20} {}\".format(ent.label_,ent))\n",
    "print()\n",
    "## TODO output labeled entities (in \"entities\" dictionary of resume)\n",
    "print(\"LABELED:\")\n",
    "for ent in resume[1][\"entities\"]:\n",
    "    print(\"{:20} {}\".format(ent[2],resume[0][ent[0]:ent[1]]))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics for NER\n",
    "Now that we can predict entities using our trained model, we can compare our predictions with the original annotations in our training data to evaluate how well our model performs for our task. The original annotations have been annotated manually by human annotators, and represent a \"Gold Standard\" against which we can compare our predictions. \n",
    "\n",
    "For a simple classification task, the most common evaluation metrics are:\n",
    "* accuracy\n",
    "* precision\n",
    "* recall\n",
    "* f1 score\n",
    "\n",
    "In order to understand these metrics, we need to understand the following concepts:\n",
    "* True positives - How many of the predicted entities are \"true\" according to the Gold Standard? (training annotation) \n",
    "* True negatives - How many entities did the model not predict which are actually not entities according to the Gold Standard?\n",
    "* False positives - How many entities did the model predict which are NOT entities according to the Gold Standard?  \n",
    "* False negatives - How many entities did the model \"miss\" - e.g. did not recognize as entities which are entities according to the Gold Standard? \n",
    "\n",
    "Before we go on, it is important that you understand true/false positives/negatives as well as the evaluation metrics above. Take some time now to research the web in order to find answers to the following questions:\n",
    "\n",
    "##### Question: How are the evaluation metrics above defined in the context of evaluating Machine Learning models? How do they relate to True/False Positives/Negatives above? Please provide an intuitive description as well as the mathmatical formula for each metric. \n",
    "*Answers here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating Metrics based on token-level annotations or full entity-level. \n",
    "The concepts above are our first step toward understanding how to evaluate our model effectively. However, in NER, we need to take into account that we can calculate our metrics either based on all tokens (words) found in the document, or only on the entities found in the document.  \n",
    "\n",
    "##### Token-Level evaluation. \n",
    "Token level evaluation evaluates how accurately did the model tag *each individual word/token* in the input. In order to understand this, we need to understand something called the \"BILUO\" Scheme (or BILOU or BIO). The spacy docs have a good reference. Please read and familiarize yourself with BILUO. \n",
    "\n",
    "https://spacy.io/api/annotation#biluo\n",
    "\n",
    "Up to now, we have not been working with the BILUO scheme, but with \"offsets\" (for example: (112,150,\"Email\") - which says there is an \"Email\" entity between positions 112 and 150 in the text). We would like to be able to evaluate our models on a token-level using BILUO - so we need to convert our data to BILUO. Fortunately, Spacy provides a helper method to do this for us.\n",
    "\n",
    "*Execute the code below to see how our \"Gold Standard\" and predictions for our example doc above look in BILUO scheme.* \n",
    "Note: some of the lines might be ommited for display purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bhupesh</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singh</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manager</td>\n",
       "      <td>U-Designation</td>\n",
       "      <td>B-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>I-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sales</td>\n",
       "      <td>O</td>\n",
       "      <td>L-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dion</td>\n",
       "      <td>B-Companies worked at</td>\n",
       "      <td>B-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Global</td>\n",
       "      <td>I-Companies worked at</td>\n",
       "      <td>I-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Solutions</td>\n",
       "      <td>I-Companies worked at</td>\n",
       "      <td>I-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ltd</td>\n",
       "      <td>L-Companies worked at</td>\n",
       "      <td>L-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Navi</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mumbai</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Email</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>me</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Indeed</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>indeed.com</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>/</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>r</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Bhupesh-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\\n</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Singh/89985037448d838f</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>L-Degree</td>\n",
       "      <td>L-Degree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>Ewing</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Christian</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>College</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Allahabad</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>Uttar</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Pradesh</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>1999</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>2002</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>ADDITIONAL</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>INFORMATION</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>persistent</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>nature</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>communication</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>skills</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>clarity</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>thought</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>622 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Tokens              Predicted                   True\n",
       "0                   Bhupesh                      O                      O\n",
       "1                     Singh                      O                      O\n",
       "2                        \\n                      O                      O\n",
       "3                   Manager          U-Designation          B-Designation\n",
       "4                         -                      O          I-Designation\n",
       "5                     Sales                      O          L-Designation\n",
       "6                         -                      O                      O\n",
       "7                      Dion  B-Companies worked at  B-Companies worked at\n",
       "8                    Global  I-Companies worked at  I-Companies worked at\n",
       "9                 Solutions  I-Companies worked at  I-Companies worked at\n",
       "10                      Ltd  L-Companies worked at  L-Companies worked at\n",
       "11                     \\n\\n                      O                      -\n",
       "12                     Navi                      O                      O\n",
       "13                   Mumbai                      O                      O\n",
       "14                        ,                      O                      O\n",
       "15              Maharashtra                      O                      O\n",
       "16                        -                      O                      O\n",
       "17                    Email                      O                      O\n",
       "18                       me                      O                      O\n",
       "19                       on                      O                      O\n",
       "20                   Indeed                      O                      O\n",
       "21                        :                      O                      O\n",
       "22               indeed.com                      O                      O\n",
       "23                        /                      O                      O\n",
       "24                        r                      O                      O\n",
       "25                        /                      O                      O\n",
       "26                 Bhupesh-                      O                      O\n",
       "27                       \\n                      O                      O\n",
       "28   Singh/89985037448d838f                      O                      O\n",
       "29                     \\n\\n                      O                      O\n",
       "..                      ...                    ...                    ...\n",
       "592             Electronics               L-Degree               L-Degree\n",
       "593                \\n\\n\\n\\n                      O                      O\n",
       "594                   Ewing                      O                      O\n",
       "595               Christian                      O                      O\n",
       "596                 College                      O                      O\n",
       "597                       -                      O                      O\n",
       "598                                              O                      O\n",
       "599               Allahabad                      O                      O\n",
       "600                       ,                      O                      O\n",
       "601                   Uttar                      O                      O\n",
       "602                 Pradesh                      O                      O\n",
       "603                    \\n\\n                      O                      O\n",
       "604                    1999                      O                      O\n",
       "605                      to                      O                      O\n",
       "606                    2002                      O                      O\n",
       "607                    \\n\\n                      O                      O\n",
       "608              ADDITIONAL                      O                      O\n",
       "609             INFORMATION                      O                      O\n",
       "610                    \\n\\n                      O                      O\n",
       "611              persistent                      O                      O\n",
       "612                  nature                      O                      O\n",
       "613                       ,                      O                      O\n",
       "614           communication                      O                      O\n",
       "615                  skills                      O                      O\n",
       "616                       ,                      O                      O\n",
       "617                       &                      O                      O\n",
       "618                 clarity                      O                      O\n",
       "619                      of                      O                      O\n",
       "620                 thought                      O                      O\n",
       "621                       .                      O                      O\n",
       "\n",
       "[622 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy.gold import biluo_tags_from_offsets\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "## returns a pandas dataframe with tokens, prediction, and true (Gold Standard) annotations of tokens\n",
    "def make_bilou_df(nlp,resume):\n",
    "    doc = nlp(resume[0])\n",
    "    bilou_ents_predicted = biluo_tags_from_offsets(doc, [(ent.start_char,ent.end_char,ent.label_)for ent in doc.ents])\n",
    "    bilou_ents_true = biluo_tags_from_offsets(doc,\n",
    "                                                   [(ent[0], ent[1], ent[2]) for ent in resume[1][\"entities\"]])\n",
    "\n",
    "    \n",
    "    doc_tokens = [tok.text for tok in doc]\n",
    "    bilou_df = pd.DataFrame()\n",
    "    bilou_df[\"Tokens\"] =doc_tokens\n",
    "    bilou_df[\"Tokens\"] = bilou_df[\"Tokens\"].str.replace(\"\\\\s+\",\"\")\n",
    "    bilou_df[\"Predicted\"] = bilou_ents_predicted\n",
    "    bilou_df[\"True\"] = bilou_ents_true\n",
    "    return bilou_df\n",
    "\n",
    "bilou_df = make_bilou_df(custom_nlp,test[110])\n",
    "display(bilou_df)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this output, it should be very easy to calculate a token-level accuracy. We simply compare the \"Predicted\" to \"True\" columns and calculate what percentage are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on one resume:  0.9662379421221865\n"
     ]
    }
   ],
   "source": [
    "## TODO bilou_df is a pandas dataframe. Use pandas dataframe api to get a subset where predicted and true are the same. \n",
    "same_df = bilou_df[bilou_df[\"Predicted\"]==bilou_df[\"True\"]]\n",
    "## accuracy is the length of this subset divided by the length of bilou_df\n",
    "accuracy = float(same_df.shape[0])/bilou_df.shape[0]\n",
    "print(\"Accuracy on one resume: \",accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy might seem pretty good... if it is not 100%, then let's print out those tokens where the model predicted something different than the gold standard by running the code below. \n",
    "\n",
    "Note - if your score on one doc is 100%, pick another document and re-run the last few cells above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manager</td>\n",
       "      <td>U-Designation</td>\n",
       "      <td>B-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>I-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sales</td>\n",
       "      <td>O</td>\n",
       "      <td>L-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Manager</td>\n",
       "      <td>U-Designation</td>\n",
       "      <td>B-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>I-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Sales</td>\n",
       "      <td>O</td>\n",
       "      <td>L-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Manager</td>\n",
       "      <td>U-Designation</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>Capital</td>\n",
       "      <td>O</td>\n",
       "      <td>B-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>Market</td>\n",
       "      <td>O</td>\n",
       "      <td>I-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>Publishers</td>\n",
       "      <td>O</td>\n",
       "      <td>I-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>India</td>\n",
       "      <td>O</td>\n",
       "      <td>I-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Private</td>\n",
       "      <td>O</td>\n",
       "      <td>I-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>Limited</td>\n",
       "      <td>O</td>\n",
       "      <td>L-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>Designation</td>\n",
       "      <td>O</td>\n",
       "      <td>B-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>I-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>Manager</td>\n",
       "      <td>U-Designation</td>\n",
       "      <td>I-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>I-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>Marketing</td>\n",
       "      <td>O</td>\n",
       "      <td>L-Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>Marketing</td>\n",
       "      <td>B-Designation</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>Executive</td>\n",
       "      <td>L-Designation</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Tokens      Predicted                   True\n",
       "3        Manager  U-Designation          B-Designation\n",
       "4              -              O          I-Designation\n",
       "5          Sales              O          L-Designation\n",
       "11          \\n\\n              O                      -\n",
       "150      Manager  U-Designation          B-Designation\n",
       "151            -              O          I-Designation\n",
       "152        Sales              O          L-Designation\n",
       "318      Manager  U-Designation                      O\n",
       "326      Capital              O  B-Companies worked at\n",
       "327       Market              O  I-Companies worked at\n",
       "328   Publishers              O  I-Companies worked at\n",
       "329        India              O  I-Companies worked at\n",
       "330      Private              O  I-Companies worked at\n",
       "331      Limited              O  L-Companies worked at\n",
       "345  Designation              O          B-Designation\n",
       "346            -              O          I-Designation\n",
       "347      Manager  U-Designation          I-Designation\n",
       "348            -              O          I-Designation\n",
       "349    Marketing              O          L-Designation\n",
       "519    Marketing  B-Designation                      O\n",
       "520    Executive  L-Designation                      O"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## find all rows in bilou_df where \"Predicted\" not equal to \"True\" column. \n",
    "diff_df = bilou_df[bilou_df[\"Predicted\"]!=bilou_df[\"True\"]]\n",
    "display(diff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the accuracy on all our test resumes and average them for an accuracy score. \n",
    "\n",
    "Please complete the code below to report an accuracy score on our test resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9686317027177864\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "doc_accuracy = []\n",
    "for tres in test:\n",
    "    tres_df = make_bilou_df(custom_nlp,tres)\n",
    "    same_df = tres_df[tres_df[\"Predicted\"]==tres_df[\"True\"]]\n",
    "    ## accuracy is len of same_df/len of bilou_df\n",
    "    accuracy = float(same_df.shape[0])/tres_df.shape[0]\n",
    "    doc_accuracy.append(accuracy)\n",
    "\n",
    "total_acc = np.mean(doc_accuracy)\n",
    "print(\"Accuracy: \",total_acc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: how does the model perform on token-level accuracy? What did it miss? In those cases where the predictions didn't match the gold standard, were the predictions plausible or just \"spurious\" (wrong)? \n",
    "*Answer here* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: What might the advantages and disadvantages be of calculating accuracy on token-level? Hint: think about a document with 1000 tokens where only 10 tokens are annotated as entities. What might the accuracy be on such a document?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entity-Level evaluation #####\n",
    "Another method of evaluating the performance of our NER model is to calculate metrics not on token-level, but on entity level. There is a good blog article that describes this method. \n",
    "\n",
    "http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n",
    "\n",
    "The article goes into some detail, the most important part is the scenarios described in the section \"Comparing NER system output and golden standard\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: how do the first 3 scenarios described in the section \"Comparing NER system output and golden standard\" correlate to  true/false positives/negatives? \n",
    "*Answer here* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision, Recall, F1 #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to calculate precision, recall, and f1 for each entity type we are interested in (our chosen entities). To do this, we need to understand the formulas for each. A good article for this is https://skymind.ai/wiki/accuracy-precision-recall-f1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: how can we calculate precision, recall and f1 score based on the information above? Please provide the formulas for each #####\n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now supply code below which calculates precision and recall and F1 on our test data for each entity type we are interested in. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For label 'Companies worked at' tp: 1177 fp: 579 fn: 369\n",
      "For label 'Degree' tp: 699 fp: 393 fn: 120\n",
      "For label 'Designation' tp: 1150 fp: 736 fn: 311\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Companies worked at</th>\n",
       "      <td>0.670273</td>\n",
       "      <td>0.761320</td>\n",
       "      <td>0.712901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Degree</th>\n",
       "      <td>0.640110</td>\n",
       "      <td>0.853480</td>\n",
       "      <td>0.731554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Designation</th>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.787132</td>\n",
       "      <td>0.687183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Precision    Recall        f1\n",
       "Companies worked at   0.670273  0.761320  0.712901\n",
       "Degree                0.640110  0.853480  0.731554\n",
       "Designation           0.609756  0.787132  0.687183"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO cycle through chosen_entity_labels and calculate metrics for each entity using test data\n",
    "## Tip: use make_bilou_df on each resume in our test set, and calculate for each entity true and false positives, and false negatives. \n",
    "## Then use the formulas you learned to calculate metrics and print them out\n",
    "## Also - store the precisions, recalls, and f1s for each entity in a data structure to access later. \n",
    "data = []\n",
    "for label in chosen_entity_labels:\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for tres in test:\n",
    "        tres_df = make_bilou_df(custom_nlp,tres)\n",
    "        tp = tres_df[(tres_df[\"Predicted\"]==tres_df[\"True\"]) & (tres_df[\"Predicted\"].str.contains(label))]\n",
    "        fp = tres_df[(tres_df[\"Predicted\"]!=tres_df[\"True\"]) & (tres_df[\"Predicted\"].str.contains(label))]\n",
    "        fn = tres_df[(tres_df[\"Predicted\"]!=tres_df[\"True\"]) & (tres_df[\"True\"].str.contains(label))]\n",
    "        true_positives += tp.shape[0]\n",
    "        false_positives += fp.shape[0]\n",
    "        false_negatives += fn.shape[0]\n",
    "        \n",
    "\n",
    "    print(\"For label '{}' tp: {} fp: {} fn: {}\".format(label,true_positives,false_positives,false_negatives))\n",
    "    \n",
    "    precision = 0.0 if true_positives==0 else float(true_positives)/(true_positives+false_positives)\n",
    "    recall = 0.0 if true_positives==0 else float(true_positives)/(true_positives+false_negatives)\n",
    "    f1 = 0.0 if precision+recall==0 else 2*((precision*recall)/(precision+recall))\n",
    "    #print(\"Precision: \",precision)\n",
    "    #print(\"Recall: \",recall)\n",
    "    #print(\"F1: \",f1)\n",
    "    row = [precision,recall,f1]\n",
    "    data.append(row)\n",
    "\n",
    "metric_df = pd.DataFrame(data,index=chosen_entity_labels,columns=[\"Precision\",\"Recall\",\"f1\"])\n",
    "display(metric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you stored the individual metrics in a data structure or variables, you can easily average the precision, recall, and f1 for all entities to compute an average score for each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision:  0.6400464453967426\n",
      "Average Recall:  0.8006438296874503\n",
      "Average f1:  0.7105459945402233\n"
     ]
    }
   ],
   "source": [
    "## TODO compute average metrics\n",
    "print(\"Average Precision: \",metric_df[\"Precision\"].mean())\n",
    "print(\"Average Recall: \",metric_df[\"Recall\"].mean())\n",
    "print(\"Average f1: \",metric_df[\"f1\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: how do the average metrics here (computed on entity-level) compare to the token-level accuracy score above? Which metric(s) would you prefer to use to evaluate the quality of your model? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost Done with part II! We just need to save our BILUO training data for reuse in Part III. Complete the code below to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO persist BILUO data as text\n",
    "def full_biluo_df(nlp,train,test):\n",
    "    ## TODO persist BILUO data as text\n",
    "    print(\"Make bilou dfs\")\n",
    "    training_data_as_bilou = [make_bilou_df(nlp,res) for res in train]\n",
    "    test_data_as_bilou = [make_bilou_df(nlp,res) for res in test]\n",
    "    print(\"Done!\")\n",
    "    training_df = pd.DataFrame(columns = [\"text\",\"ner\",\"doc\",\"ner_spacy\"])\n",
    "    test_df = pd.DataFrame(columns = [\"text\",\"ner\",\"doc\",\"ner_spacy\"])\n",
    "    for idx,df in enumerate(training_data_as_bilou):\n",
    "        df2 = pd.DataFrame()\n",
    "        df2[\"text\"] = df[\"Tokens\"].str.replace(\"\\\\s+\",\"\")\n",
    "        df2[\"ner\"] = df[\"True\"]\n",
    "        df2[\"ner_spacy\"]=df[\"Predicted\"]\n",
    "        df2[\"doc\"]=idx\n",
    "        training_df = training_df.append(df2)\n",
    "    for idx,df in enumerate(test_data_as_bilou):\n",
    "        df2 = pd.DataFrame()\n",
    "        df2[\"text\"] = df[\"Tokens\"].str.replace(\"\\\\s+\",\"\")\n",
    "        df2[\"ner\"] = df[\"True\"]\n",
    "        df2[\"ner_spacy\"]=df[\"Predicted\"]\n",
    "        df2[\"doc\"]=idx\n",
    "        test_df = test_df.append(df2)\n",
    "    return training_df,test_df\n",
    "\n",
    "train_df,test_df = full_biluo_df(custom_nlp,train,test)\n",
    "\n",
    "with open(\"../dataset/flair/train_res_bilou.txt\",'w+',encoding=\"utf-8\") as f:\n",
    "    train_df.to_csv(f,sep=\" \",encoding=\"utf-8\",index=False)\n",
    "\n",
    "with open(\"../dataset/flair/test_res_bilou.txt\",'w+',encoding=\"utf-8\") as f:\n",
    "    test_df.to_csv(f,sep=\" \",encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to see if we can load the data using flair nlp before we go on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
